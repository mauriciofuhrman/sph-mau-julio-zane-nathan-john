CrayPat/X:  Version 23.12.0 Revision 67ffc52e7 sles15.4_x86_64  11/13/23 21:04:20

Number of PEs (MPI ranks):     1
                           
Numbers of PEs per Node:       1
                           
Numbers of Threads per PE:   256
                           
Number of Cores per Socket:   64

Execution start time:  Mon Oct 21 17:37:26 2024

System name and speed:  nid004180  2.448 GHz (nominal)

AMD   Milan                CPU  Family: 25  Model:  1  Stepping:  1

Core Performance Boost:  256 PEs have CPB capability

Current path to data file:
  /global/u1/j/jkowal/sph-mau-julio-zane-nathan-john/sph.x+pat+1805921-1151977387s   (RTS)


Notes for table 1:

  This table shows functions that have significant exclusive sample
    hits, averaged across ranks.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O samp_profile ...

Table 1:  Sample Profile by Function

  Samp% |     Samp |    Imb. |  Imb. | Group
        |          |    Samp | Samp% |  Function
        |          |         |       |   Thread=HIDE
       
 100.0% | 11,807.0 |      -- |    -- | Total
|--------------------------------------------------------
|  56.1% |  6,626.0 |      -- |    -- | OMP
||-------------------------------------------------------
||  35.0% |  4,132.0 |      -- |    -- | omp_fulfill_event
||  20.0% |  2,363.0 | 1,121.0 | 29.9% | omp_test_nest_lock
||   1.1% |    128.0 |   131.4 | 40.4% | omp_get_num_procs
||=======================================================
|  23.2% |  2,742.0 |      -- |    -- | USER
||-------------------------------------------------------
||   9.9% |  1,170.0 |      -- |    -- | leapfrog_step
||   6.0% |    706.0 |   499.9 | 37.9% | compute_accel
||   5.7% |    669.0 |   510.6 | 37.7% | compute_density
||=======================================================
|  16.6% |  1,964.0 |      -- |    -- | ETC
||-------------------------------------------------------
||  14.8% |  1,748.0 |      -- |    -- | GOMP_parallel
||   1.4% |    165.0 |      -- |    -- | __pat_memset
||=======================================================
|   3.8% |    443.0 |      -- |    -- | STDIO
||-------------------------------------------------------
||   3.7% |    438.0 |      -- |    -- | fprintf
|========================================================

Notes for table 2:

  This table shows functions that have the most significant exclusive
    time, taking the maximum time across ranks and threads.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O profile_max ...

Table 2:  Profile of maximum function times

  Samp% |    Samp |    Imb. |  Imb. | Function
        |         |    Samp | Samp% |  Thread=HIDE
|----------------------------------------------------------
| 100.0% | 9,837.0 | 2,815.1 | 28.7% | clone
|  42.0% | 4,132.0 |      -- |    -- | omp_fulfill_event
|  38.2% | 3,760.0 | 1,121.0 | 29.9% | omp_test_nest_lock
|  17.8% | 1,748.0 |      -- |    -- | GOMP_parallel
|  13.8% | 1,358.0 |   510.6 | 37.7% | compute_density
|  13.5% | 1,325.0 |   499.9 | 37.9% | compute_accel
|  11.9% | 1,170.0 |      -- |    -- | leapfrog_step
|   4.5% |   438.0 |      -- |    -- | fprintf
|   3.3% |   327.0 |   131.4 | 40.4% | omp_get_num_procs
|   1.7% |   165.0 |      -- |    -- | __pat_memset
|   1.3% |   123.0 |    61.7 | 50.4% | hash_particles
|   1.2% |   122.0 |    44.4 | 36.5% | particle_neighborhood
|==========================================================

Notes for table 3:

  This table shows functions that have the most significant exclusive
    time, taking for each thread the average time across ranks.
    The imbalance percentage is relative to the team observed to
    participate in execution.
    Use -s th=ALL to see individual thread values.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O profile_th_pe ...

Table 3:  Profile by Function Group and Function with Ranks under Threads

  Samp% |     Samp |    Imb. |  Imb. | Team | Group
        |          |    Samp | Samp% | Size |  Function
        |          |         |       |      |   Thread=HIDE
        |          |         |       |      |    PE=HIDE
       
 100.0% | 11,807.0 |      -- |    -- |   -- | Total
|---------------------------------------------------------------
|  56.1% |  6,626.0 |      -- |    -- |   -- | OMP
||--------------------------------------------------------------
||  35.0% |  4,132.0 |      -- |    -- |    1 | omp_fulfill_event
||  20.0% |  2,363.0 | 1,121.0 | 29.9% |  256 | omp_test_nest_lock
||   1.1% |    128.0 |   131.4 | 40.4% |  256 | omp_get_num_procs
||==============================================================
|  23.2% |  2,742.0 |      -- |    -- |   -- | USER
||--------------------------------------------------------------
||   9.9% |  1,170.0 |      -- |    -- |    1 | leapfrog_step
||   6.0% |    706.0 |   499.9 | 37.9% |  256 | compute_accel
||   5.7% |    669.0 |   510.6 | 37.7% |  256 | compute_density
||==============================================================
|  16.6% |  1,964.0 |      -- |    -- |   -- | ETC
||--------------------------------------------------------------
||  14.8% |  1,748.0 |      -- |    -- |    1 | GOMP_parallel
||   1.4% |    165.0 |      -- |    -- |    1 | __pat_memset
||==============================================================
|   3.8% |    443.0 |      -- |    -- |   -- | STDIO
||--------------------------------------------------------------
||   3.7% |    438.0 |      -- |    -- |    1 | fprintf
|===============================================================

Notes for table 4:

  This table shows functions, and line numbers within functions, that
    have significant exclusive sample hits, averaged across ranks.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O samp_profile+src ...

Table 4:  Sample Profile by Group, Function, and Line

  Samp% |     Samp |    Imb. |  Imb. | Group
        |          |    Samp | Samp% |  Function
        |          |         |       |   Source
        |          |         |       |    Line
        |          |         |       |     Thread=HIDE
       
 100.0% | 11,807.0 |      -- |    -- | Total
|-----------------------------------------------------------------------------
|  56.1% |  6,626.0 |      -- |    -- | OMP
||----------------------------------------------------------------------------
||  35.0% |  4,132.0 |      -- |    -- | omp_fulfill_event
||  20.0% |  2,363.0 | 1,121.0 | 29.9% | omp_test_nest_lock
||   1.1% |    128.0 |   131.4 | 40.4% | omp_get_num_procs
||============================================================================
|  23.2% |  2,742.0 |      -- |    -- | USER
||----------------------------------------------------------------------------
||   9.9% |  1,170.0 |      -- |    -- | leapfrog_step
3|   9.0% |  1,065.0 |      -- |    -- |  j/jkowal/sph-mau-julio-zane-nathan-john/vec3.hpp
4|   8.7% |  1,029.0 |      -- |    -- |   line.55
||   6.0% |    706.0 |      -- |    -- | compute_accel
3|   5.3% |    625.0 |      -- |    -- |  j/jkowal/sph-mau-julio-zane-nathan-john/interact.cpp
||||--------------------------------------------------------------------------
4|||   1.5% |    180.0 |   141.4 | 45.5% | line.249
4|||   3.4% |    396.0 |   395.9 | 45.4% | line.251
||||==========================================================================
||   5.7% |    669.0 |      -- |    -- | compute_density
|||---------------------------------------------------------------------------
3||   3.9% |    465.0 |      -- |    -- | j/jkowal/sph-mau-julio-zane-nathan-john/vec3.hpp
4||   3.7% |    431.0 |   441.5 | 45.6% |  line.40
3||   1.7% |    204.0 |      -- |    -- | j/jkowal/sph-mau-julio-zane-nathan-john/interact.cpp
4||   1.6% |    189.0 |   127.4 | 37.6% |  line.69
||============================================================================
|  16.6% |  1,964.0 |      -- |    -- | ETC
||----------------------------------------------------------------------------
||  14.8% |  1,748.0 |      -- |    -- | GOMP_parallel
||   1.4% |    165.0 |      -- |    -- | __pat_memset
||============================================================================
|   3.8% |    443.0 |      -- |    -- | STDIO
||----------------------------------------------------------------------------
||   3.7% |    438.0 |      -- |    -- | fprintf
|=============================================================================

Notes for table 5:

  This table shows HW performance counter data for the whole program,
    averaged across ranks or threads, as applicable.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O hwpc ...

Table 5:  Program HW Performance Counter Data

Thread=HIDE

  
==============================================================================
  Total
------------------------------------------------------------------------------
  Thread Time                                                  118.552275 secs
  REQUESTS_TO_L2_GROUP1:L2_HW_PF                              488,327,737 
  REQUESTS_TO_L2_GROUP1:RD_BLK_X                              104,246,283 
  CORE_TO_L2_CACHEABLE_REQUEST_ACCESS_STATUS:LS_RD_BLK_C      193,960,862 
  L2_PREFETCH_HIT_L2:L2_HW_PREFETCHER:L1_HW_PREFETCHER        911,915,488 
  L2_PREFETCH_HIT_L3:L2_HW_PREFETCHER:L1_HW_PREFETCHER        116,627,217 
==============================================================================

Notes for table 6:

  This table show the average time and number of bytes written to each
    output file, taking the average over the number of ranks that
    wrote to the file.  It also shows the number of write operations,
    and average rates.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O write_stats ...

Table 6:  File Output Stats by Filename

      Avg | Avg Write |  Write Rate | Number |  Avg Writes | Bytes/ | File Name=!x/^/(proc|sys)/
    Write |   MiBytes | MiBytes/sec |     of |  per Writer |   Call |  PE=HIDE
 Time per |       per |             | Writer |        Rank |        | 
   Writer |    Writer |             |  Ranks |             |        | 
     Rank |      Rank |             |        |             |        | 
|-----------------------------------------------------------------------------
| 3.494681 | 50.285966 |   14.389286 |      1 | 1,352,001.0 |  39.00 | run.out
| 0.027897 |  0.009798 |    0.351228 |      1 |       401.0 |  25.62 | stdout
|=============================================================================

Notes for table 7:

  This table shows energy and power usage for the nodes with the
    maximum, mean, and minimum usage, as well as the sum of usage over
    all nodes.
    Energy and power for accelerators is also shown, if available.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O program_energy ...

Table 7:  Program Energy and Power Usage from Cray PM

Thread=HIDE

  
============================================================
  Total
------------------------------------------------------------
  PM Energy Node    414 W     49,086 J
  PM Energy Cpu     206 W     24,390 J
  PM Energy Memory  139 W     16,519 J
  Process Time            118.571114 secs
============================================================

Notes for table 8:

  This table shows values shown for HiMem calculated from information
    in the /proc/self/numa_maps files captured near the end of the
    program. It is the total size of all pages, including huge pages,
    that were actually mapped into physical memory from both private
    and shared memory segments.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O himem ...

Table 8:  Memory High Water Mark by Numa Node

Numanode

  
==============================================================================
  numanode.1
------------------------------------------------------------------------------
  Process HiMem (MiBytes)          2,372.0 
  HiMem Numa Node 0 (MiBytes)        269.0 MiBytes
  HiMem Numa Node 1 (MiBytes)        539.1 MiBytes
  HiMem Numa Node 2 (MiBytes)        234.1 MiBytes
  HiMem Numa Node 3 (MiBytes)        254.8 MiBytes
  HiMem Numa Node 4 (MiBytes)        250.4 MiBytes
  HiMem Numa Node 5 (MiBytes)        282.8 MiBytes
  HiMem Numa Node 6 (MiBytes)        253.2 MiBytes
  HiMem Numa Node 7 (MiBytes)        288.5 MiBytes
==============================================================================

Notes for table 9:

  This table shows memory traffic for numa nodes, taking for each numa
    node the maximum value across nodes. It also shows the balance in
    memory traffic by showing the top 3 and bottom 3 node values.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O mem_bw ...

Table 9:  Memory Bandwidth by Numanode

     Thread | Numanode
       Time |  Thread=HIDE
|-------------------------
| 117.811509 | numanode.0
| 118.552275 | numanode.1
| 118.093042 | numanode.2
| 118.018008 | numanode.3
| 118.081511 | numanode.4
| 118.070658 | numanode.5
| 117.711258 | numanode.6
| 118.073953 | numanode.7
|=========================

Notes for table 10:

  This table shows total wall clock time for the ranks with the
    maximum, mean, and minimum time, as well as the average across
    ranks.
    It also shows maximum memory usage from /proc/self/numa_maps for
    those ranks, and on average.  The usage is total size of all
    pages, including huge pages, that were actually mapped into
    physical memory from both private and shared memory segments.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O program_time ...

Table 10:  Wall Clock Time, Memory High Water Mark

    Process |   Process | Thread
       Time |     HiMem | 
            | (MiBytes) | 
           
 118.571114 |   2,372.0 | Total
|---------------------------------
| 118.571114 |   2,372.0 | thread.0
|=================================

========================  Additional details  ========================

General table notes:

    The default notes for a table are based on the default definition of
    the table, and do not account for the effects of command-line options
    that may modify the content of the table.
    
    Detailed notes, produced by the pat_report -v option, do account for
    all command-line options, and also show how data is aggregated, and
    if the table content is limited by thresholds, rank selections, etc.
    
    An imbalance metric in a line is based on values in main threads
    across multiple ranks, or on values across all threads, as applicable.
    
    An imbalance percent in a line is relative to the maximum value
    for that line across ranks or threads, as applicable.
    
    If the number of Calls for a function is shown as "--", then that
    function was not traced and the other values in its line summarize
    the data collected for functions that it calls and that were traced.
    
Experiment:  samp_cs_time

Sampling interval:  10000 microsecs

Original path to data file:
  /global/u1/j/jkowal/sph-mau-julio-zane-nathan-john/sph.x+pat+1805921-1151977387s/xf-files   (RTS)

Original program:
  /global/u1/j/jkowal/sph-mau-julio-zane-nathan-john/sph.x

Instrumented with:  pat_build sph.x

  Option file "apa" contained:
    -Drtenv=PAT_RT_PERFCTR=default_samp
    -Drtenv=PAT_RT_EXPERIMENT=samp_cs_time
    -Drtenv=PAT_RT_SAMPLING_MODE=3
    -g upc
    -g caf
    -g mpi
    -g shmem
    -g syscall
    -g io
    -g dl

Instrumented program:  ./sph.x+pat

Program invocation:  ./sph.x+pat

Exit Status:  0 for 1 PE

Thread start functions and creator functions:
     1 thread:  main
   255 threads:  omp_fulfill_event <- omp_fulfill_event

Memory pagesize:  4 KiB

Memory hugepagesize:  Not Available

Programming environment:  GNU

Runtime environment variables:
  CRAYPAT_COMPILER_OPTIONS=1
  CRAYPAT_LD_LIBRARY_PATH=/opt/cray/pe/perftools/23.12.0/lib64
  CRAYPAT_OPTS_EXECUTABLE=libexec64/opts
  CRAYPAT_ROOT=/opt/cray/pe/perftools/23.12.0
  CRAYPE_VERSION=2.7.30
  CRAY_CUDATOOLKIT_VERSION=23.9_12.2
  CRAY_DSMML_VERSION=0.2.2
  CRAY_MPICH_VERSION=8.1.28
  CRAY_PERFTOOLS_VERSION=23.12.0
  CRAY_PE_LIBSCI_VERSION=23.12.5
  GCC_VERSION=12.3
  GNU_VERSION=12.3
  HUGETLB_VERBOSE=0
  LMOD_FAMILY_COMPILER_VERSION=12.3
  LMOD_FAMILY_CRAYPE_ACCEL_VERSION=false
  LMOD_FAMILY_CRAYPE_CPU_VERSION=false
  LMOD_FAMILY_CRAYPE_NETWORK_VERSION=false
  LMOD_FAMILY_CRAYPE_VERSION=2.7.30
  LMOD_FAMILY_CUDATOOLKIT_VERSION=12.2
  LMOD_FAMILY_GCC_COMPILER_VERSION=12.3
  LMOD_FAMILY_HARDWARE_VERSION=1.0
  LMOD_FAMILY_LIBSCI_VERSION=23.12.5
  LMOD_FAMILY_MPI_VERSION=8.1.28
  LMOD_FAMILY_PERFTOOLS_VERSION=false
  LMOD_FAMILY_PRGENV_VERSION=8.5.0
  LMOD_VERSION=8.7.31
  MPICH_DIR=/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3
  MPICH_GPU_SUPPORT_ENABLED=0
  MPICH_MPIIO_DVS_MAXNODES=24
  MPICH_OFI_CXI_COUNTER_REPORT=0
  NERSC_FAMILY_PERFTOOLS_VERSION=false
  PAT_RT_EXPERIMENT=samp_cs_time
  PAT_RT_PERFCTR=default_samp
  PAT_RT_PERFCTR_DISABLE_COMPONENTS=nvml,rocm_smi
  PAT_RT_SAMPLING_MODE=3
  PERFTOOLS_VERSION=23.12.0
  PMI_SHARED_SECRET=6410486999272095460
  TERM_PROGRAM_VERSION=1.94.2

Report time environment variables:
    CRAYPAT_ROOT=/opt/cray/pe/perftools/23.12.0

Number of MPI control variables collected:  136

  (To see the list, specify: -s mpi_cvar=show)

Report command line options:  <none>

Operating system:
  Linux 5.14.21-150400.24.111_12.0.91-cray_shasta_c #1 SMP Mon Aug 5 07:02:07 UTC 2024 (0c4e917)

Hardware performance counter events:
   REQUESTS_TO_L2_GROUP1:L2_HW_PF                          All L2 cache requests:Number of prefetches accepted by L2 pipeline, hit or miss
   REQUESTS_TO_L2_GROUP1:RD_BLK_X                          All L2 cache requests:Number of data cache stores
   CORE_TO_L2_CACHEABLE_REQUEST_ACCESS_STATUS:LS_RD_BLK_C  L2 cache request outcomes. This event does not count accesses to the L2 cache by the L2 prefetcher:Number of data cache requests missing in the L2 (all types)
   L2_PREFETCH_HIT_L2:L2_HW_PREFETCHER:L1_HW_PREFETCHER    Number of L2 prefetches that hit in the L2:Number of requests generated by L2 hardware prefetcher:Number of requests generated by L1 hardware prefetcher
   L2_PREFETCH_HIT_L3:L2_HW_PREFETCHER:L1_HW_PREFETCHER    Number of L2 prefetches accepted by the L2 pipeline which miss theL2 cache and hit the L3:Number of requests generated by L2 hardware prefetcher:Number of requests generated by L1 hardware prefetcher

